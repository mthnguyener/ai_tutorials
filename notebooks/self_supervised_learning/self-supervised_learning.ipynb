{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ffc98c-e773-402b-86a4-b8b120f640df",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning <a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd371483-72c7-4313-9c63-e92f65254e56",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Self-Supervised Learning Overview](#ssl_overview)\n",
    "    * [Core Concepts](#core_concepts)\n",
    "    * [How It Works](#how_it_works)\n",
    "* [Self-Supervised Learning Tutorial](#ssl_tutorial)\n",
    "    * [Imports](#imports)\n",
    "    * [Dataset Preparation](#dataset_prep)\n",
    "    * [Model Architecture](#model_architecture)\n",
    "    * [Projection Head](#projection_head)\n",
    "    * [Simple Contrastive Learning of Representations](#simclr)\n",
    "    * [Constrastive Loss Function](#constrastive_loss)\n",
    "    * [Init the Model](#init_model)\n",
    "    * [Training Loop](#training_loop)\n",
    "* [Downstream Task (Image Classification)](#img_classification)\n",
    "    * [Setup](#setup)\n",
    "    * [Feature Extraction](#feature_extraction)\n",
    "    * [Init the Classifier](#init_classifier)\n",
    "    * [Train the Classifier](#train_classifier)\n",
    "    * [Evaluate the Classifier](#eval_classifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76bb7b-5454-402c-86ee-eac372e3e9b8",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Overview <a class=\"anchor\" id=\"ssl_overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8b0d2-4977-4bd4-85b3-c5b8295a302b",
   "metadata": {},
   "source": [
    "Self-supervised learning (SSL) leverages unsupervised learning for tasks that conventionally require supervised learning. SSL has been gaining a lot of interests in recent years for its ability to learn from unlabeled data, reduce annotation costs, and facilitate transferable representations\n",
    "\n",
    "Instead of relying on labeled datasets to understand semantic meanings, self-supervised models generate implicit labels from unstructured data. This enables the model to extract meaningful features from the data, allowing it to learn useful representations even without explicit labels.\n",
    "\n",
    "SSL is particularly useful in fields like computer vision and natural language processing (NLP) where obtaining large amounts of labeled data can be challenging (i.e. anomaly detection).\n",
    "\n",
    "A core technique in self-supervised learning is contrastive learning which focuses on maximizing the similarity between representations of similar data points and minimizing the similarity between dissimilar ones. Imagine showing your model two images: one of a cat and another of a dog. Contrastive learning encourages the model to create representations where the cat image's representation is closer to another cat image's representation than it is to the dog image's representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ee686-c5ea-43d8-9c60-75f4b0c80541",
   "metadata": {},
   "source": [
    "## How It Works <a class=\"anchor\" id=\"how_it_works\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6547e0-23b0-4845-a961-75cde394919a",
   "metadata": {},
   "source": [
    "- In supervised learning, ground truth labels are directly provided by human experts.\n",
    "- In self-supervised learning, tasks are designed such that “ground truth” can be inferred from unlabeled data.\n",
    "- SSL tasks fall into two categories:\n",
    "  - Pretext Tasks: Train AI systems to learn meaningful representations of unstructured data. These learned representations can be subsequently used in downstream tasks.\n",
    "  - Downstream Tasks: Reuse pre-trained models on new tasks, a technique known as \"transfer learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846541c-ebfc-41e5-914e-b7971359f7ff",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Tutorial <a class=\"anchor\" id=\"ssl_tutorial\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660031e8-15b8-42d2-92d2-00945a11ddae",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8219841a-8405-43a1-b45a-17a0e4e3bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330201fa-7840-47c1-b609-1bf4ea570e3b",
   "metadata": {},
   "source": [
    "## Dataset Preparation <a class=\"anchor\" id=\"dataset_prep\"></a>\n",
    "\n",
    "For this tutorial, we'll use the CIFAR-10 dataset. You can download and load it using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fed0b5-3279-425a-b44f-0c296e216029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a033a6-adde-44a5-9681-96fb770dde6f",
   "metadata": {},
   "source": [
    "## Model Architecture <a class=\"anchor\" id=\"model_architecture\"></a>\n",
    "\n",
    "Define a simple convolutional neural network (CNN) as our base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b990dfc0-c61c-41d4-88f8-002ccfddff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb7706-8cdd-4bd1-9635-55d7e3acf10d",
   "metadata": {},
   "source": [
    "## Projection Head <a class=\"anchor\" id=\"projection_head\"></a>\n",
    "\n",
    "Add a projection head to project the encoded features into a lower-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7732fc-159b-4efd-8fd9-4122713ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3d4e7-2b80-4d5c-8e7c-3f7e005ab4ef",
   "metadata": {},
   "source": [
    "## Simple Contrastive Learning of Representations <a class=\"anchor\" id=\"simclr\"></a>\n",
    "\n",
    "Combine the encoder and projection head into the SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06eca4c-94bb-419f-9406-92e0fccfe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, projection_head):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten the features\n",
    "        projections = self.projection_head(features)\n",
    "        return features, projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ffb07-2e89-4eab-87af-6324663d6f56",
   "metadata": {},
   "source": [
    "## Contrastive Loss Function <a class=\"anchor\" id=\"constrastive_loss\"></a>\n",
    "\n",
    "Define the contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e700980-01eb-4490-b85c-eeba34c0cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, projections):\n",
    "        bs = features.size(0)\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        mask = torch.eye(bs, dtype=torch.bool).cuda()\n",
    "        loss = F.cross_entropy(similarity_matrix, torch.arange(bs).cuda())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3201bac-a482-4a78-8bef-6ac8862c10ae",
   "metadata": {},
   "source": [
    "## Init Model <a class=\"anchor\" id=\"init_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc125aea-0e3e-4b38-b70f-4481e49be0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "projection_head = ProjectionHead(256 * 4 * 4, 256, 128).to(device) # Update projection head input dimension\n",
    "model = SimCLR(encoder, projection_head).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d459a-7d75-43cc-b22e-e35399643104",
   "metadata": {},
   "source": [
    "## Training Loop <a class=\"anchor\" id=\"training_loop\"></a>\n",
    "\n",
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65da0dfb-fc0c-41fa-92e4-1d1adbcd8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.9768\n",
      "Epoch [2/10], Loss: 4.1543\n",
      "Epoch [3/10], Loss: 4.1545\n",
      "Epoch [4/10], Loss: 4.1531\n",
      "Epoch [5/10], Loss: 4.1527\n",
      "Epoch [6/10], Loss: 4.1546\n",
      "Epoch [7/10], Loss: 4.1554\n",
      "Epoch [8/10], Loss: 4.1553\n",
      "Epoch [9/10], Loss: 4.1548\n",
      "Epoch [10/10], Loss: 4.1566\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch\n",
    "        images = images.to(device)\n",
    "        features, projections = model(images)\n",
    "        loss = criterion(features, projections)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2205aa-0f23-435d-97c4-f07a2b74998a",
   "metadata": {},
   "source": [
    "# Downstream Task (Image Classification) <a class=\"anchor\" id=\"img_classification\"></a>\n",
    "\n",
    "Simple linear classifier trained on top of the frozen encoder of your SimCLR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d3fbb-11b5-4383-b572-567a70ee8964",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0de409-2f22-4197-a96c-1476569e359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load your trained SimCLR model and set it to evaluation mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Data Augmentation (Replace with your preferred transformations)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),  # Random rotation and shift\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad292411-a84b-4c5a-aa23-5731d4a34613",
   "metadata": {},
   "source": [
    "### Feature Extraction <a class=\"anchor\" id=\"feature_extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4509ada8-da12-4155-942c-222e7c9383fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the test dataset using the encoder of SimCLR\n",
    "def extract_features(data_loader, model):\n",
    "  features = []\n",
    "  labels = []\n",
    "  for images, targets in data_loader:\n",
    "    with torch.no_grad():\n",
    "      features_batch, _ = model(images.to(device))\n",
    "      features.append(features_batch)\n",
    "      labels.append(targets)\n",
    "  return torch.cat(features, dim=0), torch.cat(labels, dim=0)\n",
    "\n",
    "# Extract features from the test dataset\n",
    "test_features, test_labels = extract_features(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cfe50-02a8-4173-a9c0-253fd47821c5",
   "metadata": {},
   "source": [
    "### Init the Classifier <a class=\"anchor\" id=\"init_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b51f76-0875-4f04-9ed8-c4a7cfbe7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "  def __init__(self, input_dim, num_classes):\n",
    "    super(LinearClassifier, self).__init__()\n",
    "    self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.fc(x)\n",
    "\n",
    "# Initialize the linear classifier (assuming the encoder output is 256 * 4 * 4)\n",
    "classifier = LinearClassifier(input_dim=3072, num_classes=10).to(device)\n",
    "\n",
    "# Hyperparameter Tuning (Experiment with different learning rates and epochs)\n",
    "learning_rate = 0.0005  # Adjust based on experimentation\n",
    "num_epochs = 20  # Adjust based on experimentation\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7a05a-e40f-45f0-a37e-3a4d8d475be9",
   "metadata": {},
   "source": [
    "### Train the Classifier <a class=\"anchor\" id=\"train_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8379c25-ee98-4028-b346-295b9a24163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear classifier\n",
    "for epoch in range(num_epochs):\n",
    "  classifier.train()\n",
    "  for features, labels in train_loader:\n",
    "    # Reshape features if necessary (same as in extract_features)\n",
    "    features = features.view(features.size(0), -1).to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = classifier(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c83c0-e4ca-46f5-9c14-91fad07d4dda",
   "metadata": {},
   "source": [
    "### Evaluate the Classifier <a class=\"anchor\" id=\"eval_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b654f27-dfae-449a-ad92-d53f2c66f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca08593-132f-4efe-b022-934dc1449eef",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This tutorial provides a basic implementation of contrastive learning with SimCLR. You can further experiment by adjusting hyperparameters, using different datasets, or exploring advanced techniques like data augmentations and different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55342e07-bbbd-4515-817e-77f037dc8d98",
   "metadata": {},
   "source": [
    "**[Go to Top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
