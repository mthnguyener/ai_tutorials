{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ffc98c-e773-402b-86a4-b8b120f640df",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning <a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd371483-72c7-4313-9c63-e92f65254e56",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Self-Supervised Learning Overview](#ssl_overview)\n",
    "    * [Core Concepts](#core_concepts)\n",
    "    * [How It Works](#how_it_works)\n",
    "* [Self-Supervised Learning Tutorial](#ssl_tutorial)\n",
    "    * [Imports](#imports)\n",
    "    * [Dataset Preparation](#dataset_prep)\n",
    "    * [Model Architecture](#model_architecture)\n",
    "    * [Projection Head](#projection_head)\n",
    "    * [Simple Contrastive Learning of Representations](#simclr)\n",
    "    * [Constrastive Loss Function](#constrastive_loss)\n",
    "    * [Init the Model](#init_model)\n",
    "    * [Training Loop](#training_loop)\n",
    "* [Downstream Task (Image Classification)](#img_classification)\n",
    "    * [Setup](#setup)\n",
    "    * [Feature Extraction](#feature_extraction)\n",
    "    * [Linear Classifier](#linear_classifier)\n",
    "    * [Init the Classifier](#init_classifier)\n",
    "    * [Train the Classifier](#train_classifier)\n",
    "    * [Evaluate the Classifier](#eval_classifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76bb7b-5454-402c-86ee-eac372e3e9b8",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Overview <a class=\"anchor\" id=\"ssl_overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8b0d2-4977-4bd4-85b3-c5b8295a302b",
   "metadata": {},
   "source": [
    "Self-supervised learning (SSL) leverages unsupervised learning for tasks that conventionally require supervised learning. SSL has been gaining a lot of interests in recent years for its ability to learn from unlabeled data, reduce annotation costs, and facilitate transferable representations\n",
    "\n",
    "Instead of relying on labeled datasets to understand semantic meanings, self-supervised models generate implicit labels from unstructured data. This enables the model to extract meaningful features from the data, allowing it to learn useful representations even without explicit labels.\n",
    "\n",
    "SSL is particularly useful in fields like computer vision and natural language processing (NLP) where obtaining large amounts of labeled data can be challenging (i.e. anomaly detection).\n",
    "\n",
    "A core technique in self-supervised learning is contrastive learning which focuses on maximizing the similarity between representations of similar data points and minimizing the similarity between dissimilar ones. Imagine showing your model two images: one of a cat and another of a dog. Contrastive learning encourages the model to create representations where the cat image's representation is closer to another cat image's representation than it is to the dog image's representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ee686-c5ea-43d8-9c60-75f4b0c80541",
   "metadata": {},
   "source": [
    "## How It Works <a class=\"anchor\" id=\"how_it_works\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6547e0-23b0-4845-a961-75cde394919a",
   "metadata": {},
   "source": [
    "- In supervised learning, ground truth labels are directly provided by human experts.\n",
    "- In self-supervised learning, tasks are designed such that “ground truth” can be inferred from unlabeled data.\n",
    "- SSL tasks fall into two categories:\n",
    "  - Pretext Tasks: Train AI systems to learn meaningful representations of unstructured data. These learned representations can be subsequently used in downstream tasks.\n",
    "  - Downstream Tasks: Reuse pre-trained models on new tasks, a technique known as \"transfer learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846541c-ebfc-41e5-914e-b7971359f7ff",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Tutorial <a class=\"anchor\" id=\"ssl_tutorial\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660031e8-15b8-42d2-92d2-00945a11ddae",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8219841a-8405-43a1-b45a-17a0e4e3bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7942a9b-fdc7-43ce-8896-346115c915e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330201fa-7840-47c1-b609-1bf4ea570e3b",
   "metadata": {},
   "source": [
    "## Dataset Preparation <a class=\"anchor\" id=\"dataset_prep\"></a>\n",
    "\n",
    "For this tutorial, we'll use the CIFAR-10 dataset. You can download and load it using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fed0b5-3279-425a-b44f-0c296e216029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "class Cutout:\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = int(max(0, y - self.length // 2))\n",
    "            y2 = int(min(h, y + self.length // 2))\n",
    "            x1 = int(max(0, x - self.length // 2))\n",
    "            x2 = int(min(w, x + self.length // 2))\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "\n",
    "        return img * mask\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    Cutout(n_holes=1, length=16)  # Introduce holes in images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee2c0fe-273b-4caf-a82e-67973643bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 16\n",
    "\n",
    "# Load CIFAR-10 Train Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
    "                                 download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Load CIFAR-10 Test Dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
    "                                download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a033a6-adde-44a5-9681-96fb770dde6f",
   "metadata": {},
   "source": [
    "## Model Architecture <a class=\"anchor\" id=\"model_architecture\"></a>\n",
    "\n",
    "Define a simple convolutional neural network (CNN) as our base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b990dfc0-c61c-41d4-88f8-002ccfddff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Add additional convolutional layers\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb7706-8cdd-4bd1-9635-55d7e3acf10d",
   "metadata": {},
   "source": [
    "## Projection Head <a class=\"anchor\" id=\"projection_head\"></a>\n",
    "\n",
    "Add a projection head to project the encoded features into a lower-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7732fc-159b-4efd-8fd9-4122713ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3d4e7-2b80-4d5c-8e7c-3f7e005ab4ef",
   "metadata": {},
   "source": [
    "## Simple Contrastive Learning of Representations <a class=\"anchor\" id=\"simclr\"></a>\n",
    "\n",
    "Combine the encoder and projection head into the SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06eca4c-94bb-419f-9406-92e0fccfe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, projection_head):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten the features\n",
    "        projections = self.projection_head(features)\n",
    "        return features, projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ffb07-2e89-4eab-87af-6324663d6f56",
   "metadata": {},
   "source": [
    "## Contrastive Loss Function <a class=\"anchor\" id=\"constrastive_loss\"></a>\n",
    "\n",
    "Define the contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e700980-01eb-4490-b85c-eeba34c0cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, projections):\n",
    "        bs = features.size(0)\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        mask = torch.eye(bs, dtype=torch.bool).cuda()\n",
    "        loss = F.cross_entropy(similarity_matrix, torch.arange(bs).cuda())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3201bac-a482-4a78-8bef-6ac8862c10ae",
   "metadata": {},
   "source": [
    "## Init Model <a class=\"anchor\" id=\"init_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc125aea-0e3e-4b38-b70f-4481e49be0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "encoder = Encoder().to(device)\n",
    "projection_head = ProjectionHead(2048, 256, 128).to(device) # Update projection head input dimension\n",
    "model = SimCLR(encoder, projection_head).to(device)\n",
    "\n",
    "# Hyperparameter Tuning (Experiment with different learning rates and epochs)\n",
    "learning_rate = 0.0005  # Adjust based on experimentation\n",
    "num_epochs = 1000  # Adjust based on experimentation\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d459a-7d75-43cc-b22e-e35399643104",
   "metadata": {},
   "source": [
    "## Training Loop <a class=\"anchor\" id=\"training_loop\"></a>\n",
    "\n",
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65da0dfb-fc0c-41fa-92e4-1d1adbcd8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 4.6771\n",
      "Epoch [100/1000], Loss: 4.8399\n",
      "Epoch [150/1000], Loss: 4.8239\n",
      "Epoch [200/1000], Loss: 4.7703\n",
      "Epoch [250/1000], Loss: 4.7547\n",
      "Epoch [300/1000], Loss: 4.6278\n",
      "Epoch [350/1000], Loss: 4.8477\n",
      "Epoch [400/1000], Loss: 4.7877\n",
      "Epoch [450/1000], Loss: 4.7393\n",
      "Epoch [500/1000], Loss: 4.8495\n",
      "Epoch [550/1000], Loss: 4.8394\n",
      "Epoch [600/1000], Loss: 4.8507\n",
      "Epoch [650/1000], Loss: 4.8476\n",
      "Epoch [700/1000], Loss: 4.8485\n",
      "Epoch [750/1000], Loss: 4.8497\n",
      "Epoch [800/1000], Loss: 4.8508\n",
      "Epoch [850/1000], Loss: 4.8508\n",
      "Epoch [900/1000], Loss: 4.8508\n",
      "Epoch [950/1000], Loss: 4.8508\n",
      "Epoch [1000/1000], Loss: 4.8508\n",
      "\n",
      "Training took 86.49322359959284 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch\n",
    "        images = images.to(device)\n",
    "        features, projections = model(images)\n",
    "        loss = criterion(features, projections)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Print information every 5 epochs or at the last epoch\n",
    "    if (epoch + 1) % 50 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining took {(end - start)/60} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2205aa-0f23-435d-97c4-f07a2b74998a",
   "metadata": {},
   "source": [
    "# Downstream Task (Image Classification) <a class=\"anchor\" id=\"img_classification\"></a>\n",
    "\n",
    "Simple linear classifier trained on top of the frozen encoder of your SimCLR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d3fbb-11b5-4383-b572-567a70ee8964",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0de409-2f22-4197-a96c-1476569e359f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (projection_head): ProjectionHead(\n",
       "    (projection_head): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained SimCLR model and set it to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad292411-a84b-4c5a-aa23-5731d4a34613",
   "metadata": {},
   "source": [
    "### Feature Extraction <a class=\"anchor\" id=\"feature_extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4509ada8-da12-4155-942c-222e7c9383fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract features from the test dataset using the encoder of SimCLR\n",
    "# def extract_features(data_loader, model):\n",
    "#   features = []\n",
    "#   labels = []\n",
    "#   for images, targets in data_loader:\n",
    "#     with torch.no_grad():\n",
    "#       features_batch, _ = model(images.to(device))\n",
    "#       features.append(features_batch)\n",
    "#       labels.append(targets)\n",
    "#   return torch.cat(features, dim=0), torch.cat(labels, dim=0)\n",
    "\n",
    "# # Extract features from the test dataset\n",
    "# test_features, test_labels = extract_features(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760acc6-3839-4dc2-9aa7-10288aed6629",
   "metadata": {},
   "source": [
    "### Linear Classifier <a class=\"anchor\" id=\"linear_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7936b9-5ec1-4b93-ace1-9990495cdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # Increased hidden layer size\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)  # Dropout layer for regularization\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(256, num_classes)  # Additional hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cfe50-02a8-4173-a9c0-253fd47821c5",
   "metadata": {},
   "source": [
    "### Init the Classifier <a class=\"anchor\" id=\"init_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b51f76-0875-4f04-9ed8-c4a7cfbe7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier (assuming reduced feature dim is 192 * 4 * 4)\n",
    "classifier = LinearClassifier(input_dim=192 * 4 * 4, \n",
    "                              num_classes=10).to(device)\n",
    "\n",
    "# Hyperparameter Tuning (Experiment with different learning rates and epochs)\n",
    "learning_rate = 0.0001  # Adjust based on experimentation\n",
    "num_epochs = 100  # Adjust based on experimentation\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate, \n",
    "                             weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7a05a-e40f-45f0-a37e-3a4d8d475be9",
   "metadata": {},
   "source": [
    "### Train the Classifier <a class=\"anchor\" id=\"train_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8379c25-ee98-4028-b346-295b9a24163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 1.9865\n",
      "Epoch [10/100], Loss: 2.0728\n",
      "Epoch [15/100], Loss: 1.9549\n",
      "Epoch [20/100], Loss: 1.9938\n",
      "Epoch [25/100], Loss: 1.7308\n",
      "Epoch [30/100], Loss: 1.9159\n",
      "Epoch [35/100], Loss: 1.9260\n",
      "Epoch [40/100], Loss: 1.9332\n",
      "Epoch [45/100], Loss: 1.7531\n",
      "Epoch [50/100], Loss: 1.9723\n",
      "Epoch [55/100], Loss: 2.0345\n",
      "Epoch [60/100], Loss: 1.9939\n",
      "Epoch [65/100], Loss: 1.9165\n",
      "Epoch [70/100], Loss: 1.8173\n",
      "Epoch [75/100], Loss: 1.8681\n",
      "Epoch [80/100], Loss: 2.0231\n",
      "Epoch [85/100], Loss: 1.8092\n",
      "Epoch [90/100], Loss: 1.9763\n",
      "Epoch [95/100], Loss: 1.8566\n",
      "Epoch [100/100], Loss: 1.9622\n",
      "\n",
      "Training took 8.280885032812755 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Train the linear classifier\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    for features, labels in train_loader:\n",
    "        # Reshape features if necessary (same as in extract_features)\n",
    "        features = features.view(features.size(0), -1).to(device)\n",
    "        # print(f\"Feature Shape: {features.shape}\")\n",
    "        labels = labels.to(device)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print information every 5 epochs or at the last epoch\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining took {(end - start)/60} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c83c0-e4ca-46f5-9c14-91fad07d4dda",
   "metadata": {},
   "source": [
    "### Evaluate the Classifier <a class=\"anchor\" id=\"eval_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b654f27-dfae-449a-ad92-d53f2c66f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 33.28%\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca08593-132f-4efe-b022-934dc1449eef",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This tutorial provides a basic implementation of contrastive learning with SimCLR. You can further experiment by adjusting hyperparameters, using different datasets, or exploring advanced techniques like data augmentations and different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55342e07-bbbd-4515-817e-77f037dc8d98",
   "metadata": {},
   "source": [
    "**[Go to Top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
