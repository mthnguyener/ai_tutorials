{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ffc98c-e773-402b-86a4-b8b120f640df",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning <a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd371483-72c7-4313-9c63-e92f65254e56",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Self-Supervised Learning Overview](#ssl_overview)\n",
    "    * [Core Concepts](#core_concepts)\n",
    "    * [How It Works](#how_it_works)\n",
    "* [Self-Supervised Learning Tutorial](#ssl_tutorial)\n",
    "    * [Imports](#imports)\n",
    "    * [Dataset Preparation](#dataset_prep)\n",
    "    * [Model Architecture](#model_architecture)\n",
    "    * [Projection Head](#projection_head)\n",
    "    * [Simple Contrastive Learning of Representations](#simclr)\n",
    "    * [Constrastive Loss Function](#constrastive_loss)\n",
    "    * [Init the Model](#init_model)\n",
    "    * [Training Loop](#training_loop)\n",
    "* [Downstream Task (Image Classification)](#img_classification)\n",
    "    * [Linear Classifier](#linear_classifier)\n",
    "    * [Setup](#setup)\n",
    "    * [Init the Classifier](#init_classifier)\n",
    "    * [Train the Classifier](#train_classifier)\n",
    "    * [Evaluate the Classifier](#eval_classifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76bb7b-5454-402c-86ee-eac372e3e9b8",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Overview <a class=\"anchor\" id=\"ssl_overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8b0d2-4977-4bd4-85b3-c5b8295a302b",
   "metadata": {},
   "source": [
    "Self-supervised learning (SSL) leverages unsupervised learning for tasks that conventionally require supervised learning. SSL has been gaining a lot of interests in recent years for its ability to learn from unlabeled data, reduce annotation costs, and facilitate transferable representations\n",
    "\n",
    "Instead of relying on labeled datasets to understand semantic meanings, self-supervised models generate implicit labels from unstructured data. This enables the model to extract meaningful features from the data, allowing it to learn useful representations even without explicit labels.\n",
    "\n",
    "SSL is particularly useful in fields like computer vision and natural language processing (NLP) where obtaining large amounts of labeled data can be challenging (i.e. anomaly detection).\n",
    "\n",
    "A core technique in self-supervised learning is contrastive learning which focuses on maximizing the similarity between representations of similar data points and minimizing the similarity between dissimilar ones. Imagine showing your model two images: one of a cat and another of a dog. Contrastive learning encourages the model to create representations where the cat image's representation is closer to another cat image's representation than it is to the dog image's representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ee686-c5ea-43d8-9c60-75f4b0c80541",
   "metadata": {},
   "source": [
    "## How It Works <a class=\"anchor\" id=\"how_it_works\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6547e0-23b0-4845-a961-75cde394919a",
   "metadata": {},
   "source": [
    "- In supervised learning, ground truth labels are directly provided by human experts.\n",
    "- In self-supervised learning, tasks are designed such that “ground truth” can be inferred from unlabeled data.\n",
    "- SSL tasks fall into two categories:\n",
    "  - Pretext Tasks: Train AI systems to learn meaningful representations of unstructured data. These learned representations can be subsequently used in downstream tasks.\n",
    "  - Downstream Tasks: Reuse pre-trained models on new tasks, a technique known as \"transfer learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846541c-ebfc-41e5-914e-b7971359f7ff",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Tutorial <a class=\"anchor\" id=\"ssl_tutorial\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660031e8-15b8-42d2-92d2-00945a11ddae",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219841a-8405-43a1-b45a-17a0e4e3bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7942a9b-fdc7-43ce-8896-346115c915e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Available Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330201fa-7840-47c1-b609-1bf4ea570e3b",
   "metadata": {},
   "source": [
    "## Dataset Preparation <a class=\"anchor\" id=\"dataset_prep\"></a>\n",
    "\n",
    "The Cutout class implements a cutout augmentation technique. It takes a mask of random squares of pixels from the input images during training, to make the model more robust.\n",
    "\n",
    "For this tutorial, we'll use the CIFAR-10 dataset. You can download and load it using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fed0b5-3279-425a-b44f-0c296e216029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "class Cutout:\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Extracts the height (h) and width (w) of the input image (img) using .size()\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        # Iterates over the specified number of holes (n_holes) and \n",
    "        # selects random coordinates (x, y) within the image dimensions\n",
    "        for _ in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            # Calculates the coordinates (y1, y2, x1, x2) for the cutout region \n",
    "            # around the random coordinates (x, y)\n",
    "            y1 = int(max(0, y - self.length // 2))\n",
    "            y2 = int(min(h, y + self.length // 2))\n",
    "            x1 = int(max(0, x - self.length // 2))\n",
    "            x2 = int(min(w, x + self.length // 2))\n",
    "\n",
    "            # Updates the corresponding region in the mask to zeros, \n",
    "            # effectively creating a hole in the mask\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "\n",
    "        # The binary mask is converted into a PyTorch tensor (torch.Tensor) and \n",
    "        # expanded to match the dimensions of the input image\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "\n",
    "        return img * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650fcd7-ed2d-488e-af9b-5a4a5b1279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of transformations as input and applies them sequentially to the image\n",
    "transform = transforms.Compose([\n",
    "    # Converts the input image into a PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Random crop of the input image and then resizes it to the specified size\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Horizontally flips the input image at a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Randomly rotates the image up to 10 degrees and \n",
    "    # translates it up to 10% of the image size both vertically and horizontally.\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "    # Randomly adjusts the brightness, contrast, saturation, and hue of the input image\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    # Normalizes the image by subtracting the mean (0.5, 0.5, 0.5) from each channel and \n",
    "    # dividing by the standard deviation (0.5, 0.5, 0.5) for each channel\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    # Randomly removes rectangular regions from the input image\n",
    "    Cutout(n_holes=1, length=16)  # Introduce holes in images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2c0fe-273b-4caf-a82e-67973643bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 Datasets\n",
    "batch_size = 128 # number of samples to be processed in each batch during training and testing\n",
    "num_workers = 16 # number of subprocesses to use for parallel data loading\n",
    "\n",
    "# Creates an instance of the CIFAR-10 dataset for training\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform)\n",
    "# Creates a data loader for the training dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Creates an instance of the CIFAR-10 dataset for testing\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=transform)\n",
    "# Creates a data loader for the testing dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a033a6-adde-44a5-9681-96fb770dde6f",
   "metadata": {},
   "source": [
    "## Model Architecture <a class=\"anchor\" id=\"model_architecture\"></a>\n",
    "\n",
    "Define a simple convolutional neural network (CNN) as our base encoder. This is a convolutional neural network that encodes the input images into feature representations. It contains Conv2d, ReLU and MaxPool2d layers.\n",
    "\n",
    "- The encoder is defined as a Sequential container of convolutional layers (Conv2d), activation functions (ReLU), and max-pooling layers (MaxPool2d). This sequence of layers forms the encoder part of the neural network.\n",
    "- The first convolutional layer (Conv2d) takes input channels of size 3 (for RGB images) and outputs 64 channels. It uses a kernel size of 3x3 and padding of 1 to maintain the spatial dimensions.\n",
    "- ReLU activation functions (ReLU) are applied after each convolutional layer to introduce non-linearity.\n",
    "- Max-pooling layers (MaxPool2d) with a kernel size of 2x2 are used to downsample the spatial dimensions of the feature maps.\n",
    "- The process is repeated with increasing numbers of output channels (64, 128, 256, and finally 512) along with downsampling using max-pooling after each set of convolutional layers.\n",
    "\n",
    "The forward method defines the forward pass computation of the Encoder module. It takes an input tensor x and passes it through the layers defined in self.encoder, returning the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990dfc0-c61c-41d4-88f8-002ccfddff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb7706-8cdd-4bd1-9635-55d7e3acf10d",
   "metadata": {},
   "source": [
    "## Projection Head <a class=\"anchor\" id=\"projection_head\"></a>\n",
    "\n",
    "Add a projection head to project the encoded features into a lower-dimensional space. This takes the encoded features from the Encoder and projects them into embeddings. It contains Linear and ReLU layers.\n",
    "\n",
    "- The projection_head is defined as a Sequential container of linear layers (Linear) and activation functions (ReLU). This sequence of layers forms the projection head part of the neural network.\n",
    "- The first linear layer (Linear) takes an input of size input_dim and outputs hidden_dim. This is essentially a fully connected layer with a ReLU activation function (ReLU) applied afterward.\n",
    "- The second linear layer (Linear) then takes the output of the previous layer (of size hidden_dim) and outputs a tensor of size output_dim. No activation function is applied after this layer.\n",
    "\n",
    "The forward method defines the forward pass computation of the ProjectionHead module. It takes an input tensor x and passes it through the layers defined in self.projection_head, returning the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7732fc-159b-4efd-8fd9-4122713ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3d4e7-2b80-4d5c-8e7c-3f7e005ab4ef",
   "metadata": {},
   "source": [
    "## Simple Contrastive Learning of Representations <a class=\"anchor\" id=\"simclr\"></a>\n",
    "\n",
    "Combine the encoder and projection head into the SimCLR model. The overall model that contains the Encoder and ProjectionHead. It handles passing inputs through these and returning features and projections.\n",
    "\n",
    "- The `__init__` method initializes the SimCLR class. It first calls the `__init__` method of the parent class (`nn.Module`) using `super()`. Then, it initializes the encoder and projection_head attributes of the SimCLR class with the provided encoder and projection_head modules.\n",
    "- Encoder typically refers to the backbone neural network architecture (e.g., a CNN) that extracts features from input data, and projection_head is a neural network module that projects the extracted features into a higher-dimensional space.\n",
    "\n",
    "The `forward` method defines the forward pass computation of the SimCLR module. It takes an input tensor `x` and passes it through the `encoder` module (backbone network) to extract features.\n",
    "- After extracting features, the `features` tensor is reshaped using `.view()` to flatten it while preserving the batch size (`features.size(0)`).\n",
    "- The flattened features are then passed through the `projection_head` module to project them into a higher-dimensional space.\n",
    "- The method returns both the extracted features and their corresponding projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06eca4c-94bb-419f-9406-92e0fccfe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, projection_head):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten the features\n",
    "        projections = self.projection_head(features)\n",
    "        return features, projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ffb07-2e89-4eab-87af-6324663d6f56",
   "metadata": {},
   "source": [
    "## Contrastive Loss Function <a class=\"anchor\" id=\"constrastive_loss\"></a>\n",
    "\n",
    "Define the contrastive loss function. This calculates the loss for the contrastive learning objective. It normalizes the features, calculates the similarity matrix between samples, and uses cross entropy on this to optimize the model.\n",
    "\n",
    "- The `__init__` method initializes the `ContrastiveLoss` class. It takes an optional argument `temperature`, which defaults to `0.5`. The temperature parameter scales the similarity scores before computing the loss.\n",
    "\n",
    "The forward method defines the forward pass computation of the ContrastiveLoss module. It takes a tensor of features extracted from the model's encoder as input.\n",
    "- First, it normalizes the feature vectors along the embedding dimension (dimension 1) using `nn.functional.normalize()`. Normalization ensures that feature vectors have unit length, which is often beneficial for contrastive learning.\n",
    "- It then computes the similarity matrix by performing a matrix multiplication of the normalized feature vectors and their transposes. The resulting matrix contains pairwise cosine similarities between all feature vectors.\n",
    "- The similarity scores are divided by the temperature parameter to scale them.\n",
    "- Finally, it computes the contrastive loss using `F.cross_entropy()`. This function calculates the cross-entropy loss between the similarity scores and the labels, where the labels are simply indices from 0 to the batch size. The loss is calculated based on how well the similarity scores match the ground truth labels, where matching views from the same instance should have high similarity scores, and views from different instances should have low similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e700980-01eb-4490-b85c-eeba34c0cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        bs = features.size(0)\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        loss = F.cross_entropy(similarity_matrix, torch.arange(bs).cuda())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3201bac-a482-4a78-8bef-6ac8862c10ae",
   "metadata": {},
   "source": [
    "## Init Model <a class=\"anchor\" id=\"init_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc125aea-0e3e-4b38-b70f-4481e49be0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "encoder = Encoder().to(device)\n",
    "projection_head = ProjectionHead(2048, 256, 128).to(device) # Update projection head input dimension\n",
    "model = SimCLR(encoder, projection_head).to(device)\n",
    "\n",
    "# Hyperparameter Tuning (Experiment with different learning rates and epochs)\n",
    "learning_rate = 0.0005  # Determines the step size at which the model parameters are updated during optimization\n",
    "num_epochs = 10  # Specifies the number of times the entire dataset will pass through the model during training\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = ContrastiveLoss() # Defines the loss function for the contrastive learning task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Defines the optimizer using Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d459a-7d75-43cc-b22e-e35399643104",
   "metadata": {},
   "source": [
    "## Training Loop <a class=\"anchor\" id=\"training_loop\"></a>\n",
    "\n",
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da0dfb-fc0c-41fa-92e4-1d1adbcd8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # Iterates over batches of data from the train_loader\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch\n",
    "        # Moves images to GPU, if available\n",
    "        images = images.to(device)\n",
    "        # Passes through the model obtain both features and their projections\n",
    "        features, projections = model(images)\n",
    "        # Contrastive loss is computed using the features\n",
    "        loss = criterion(features)\n",
    "\n",
    "        # The optimizer's gradients are zeroed\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagated the loss through the network\n",
    "        loss.backward()\n",
    "        # Updates the model parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulates total loss for the epoch\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Print information every 5 epochs or at the last epoch\n",
    "    if (epoch + 1) % (num_epochs/(num_epochs/20)) == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining took {(end - start)/60} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f09f8-180d-41b8-8b4c-2986d1b4e2af",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to show/hide expected output </summary>\n",
    "    <br>\n",
    "    Epoch [10/10], Loss: 4.8454\n",
    "    <br>\n",
    "    <br>\n",
    "    \n",
    "Training took 0.8129964351654053 mins\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2205aa-0f23-435d-97c4-f07a2b74998a",
   "metadata": {},
   "source": [
    "# Downstream Task (Image Classification) <a class=\"anchor\" id=\"img_classification\"></a>\n",
    "\n",
    "Simple linear classifier trained on top of the frozen encoder of your SimCLR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760acc6-3839-4dc2-9aa7-10288aed6629",
   "metadata": {},
   "source": [
    "### Linear Classifier <a class=\"anchor\" id=\"linear_classifier\"></a>\n",
    "\n",
    "This simple linear classifier takes the learned features and classifies them into classes. It contains Linear, ReLU and Dropout layers.\n",
    "\n",
    "- `input_dim` is the dimensionality of the input features.\n",
    "- `num_classes` is the number of classes in the classification task.\n",
    "- It consists of three fully connected (linear) layers (`nn.Linear`), each followed by a ReLU activation function (`nn.ReLU`) and a dropout layer (`nn.Dropout`).\n",
    "- The first fully connected layer (`fc1`) takes input of size input_dim and outputs 512 features.\n",
    "- The second fully connected layer (`fc2`) takes 512 features as input and outputs 256 features.\n",
    "- The third fully connected layer (`fc3`) takes 256 features as input and outputs `num_classes` features, corresponding to the class scores.\n",
    "\n",
    "The forward method defines the forward pass computation of the LinearClassifier module.\n",
    "- It takes an input tensor `x` and passes it through the layers defined in the `__init__` method.\n",
    "- After each fully connected layer, ReLU activation function is applied followed by dropout for regularization.\n",
    "- The output of the last fully connected layer is returned, representing the class scores for each input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7936b9-5ec1-4b93-ace1-9990495cdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # Increased hidden layer size\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)  # Dropout layer for regularization\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(256, num_classes)  # Additional hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d3fbb-11b5-4383-b572-567a70ee8964",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0de409-2f22-4197-a96c-1476569e359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it to evaluation mode and send it to GPU\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cfe50-02a8-4173-a9c0-253fd47821c5",
   "metadata": {},
   "source": [
    "### Init the Classifier <a class=\"anchor\" id=\"init_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b51f76-0875-4f04-9ed8-c4a7cfbe7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier (assuming reduced feature dim is 192 * 4 * 4)\n",
    "classifier = LinearClassifier(input_dim=192 * 4 * 4, \n",
    "                              num_classes=10).to(device)\n",
    "\n",
    "# Hyperparameter Tuning (Experiment with different learning rates and epochs)\n",
    "learning_rate = 0.0001  # Determines the step size at which the model parameters are updated during optimization\n",
    "num_epochs = 5  # Specifies the number of times the entire dataset will pass through the model during training\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss() # Cross-entropy loss combines a softmax activation function and a negative log-likelihood loss\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate, \n",
    "                             weight_decay=0.001) # Initializes the optimizer using the Adam algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7a05a-e40f-45f0-a37e-3a4d8d475be9",
   "metadata": {},
   "source": [
    "### Train the Classifier <a class=\"anchor\" id=\"train_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8379c25-ee98-4028-b346-295b9a24163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Train the linear classifier\n",
    "for epoch in range(num_epochs):\n",
    "    # Sets the model (classifier) to training mode\n",
    "    classifier.train()\n",
    "    # Iterates over the batches of data (features and labels) from the train_loader\n",
    "    for features, labels in train_loader:\n",
    "        # Reshape the features tensor if necessary (flattens it)\n",
    "        features = features.view(features.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Performs a forward pass through the classifier modelto obtain predicted class scores for the input features\n",
    "        outputs = classifier(features)\n",
    "        # Computes the cross-entropy loss using the predicted outputs and the ground truth labels\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # The optimizer's gradients are zeroed\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagated the loss through the network\n",
    "        loss.backward()\n",
    "        # Updates the model parameters based on the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print information every 5 epochs or at the last epoch\n",
    "    if (epoch + 1) % (num_epochs/(num_epochs/20)) == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTraining took {(end - start)/60} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccf7f9-afe2-4bcb-a3b3-2a2d7b1f5e11",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to show/hide expected output </summary>\n",
    "    <br>\n",
    "    Epoch [5/5], Loss: 2.0327\n",
    "    <br>\n",
    "    <br>\n",
    "    Training took 0.3966091076533 mins\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c83c0-e4ca-46f5-9c14-91fad07d4dda",
   "metadata": {},
   "source": [
    "### Evaluate the Classifier <a class=\"anchor\" id=\"eval_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b654f27-dfae-449a-ad92-d53f2c66f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient computation during inference to save memory and computation\n",
    "with torch.no_grad():\n",
    "    # Iterates over the batches of data (images and labels) from the test_loader\n",
    "    for images, labels in test_loader:\n",
    "        # Reshape the features tensor if necessary (flattens it)\n",
    "        images = images.view(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Performs a forward pass through the classifier model to obtain predicted class scores for the input image\n",
    "        outputs = classifier(images)\n",
    "        # Uses `torch.max()` to find the maxiumum value along dimension 1 and return the predicted value\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Increments the total counter by the number of labels in the current batch\n",
    "        total += labels.size(0)\n",
    "        # Comparing the predicted labels with the ground truth labels and summing the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e496132-dd8c-4fbf-a29b-a831e23638f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to show/hide expected output </summary>\n",
    "    <br>\n",
    "    Accuracy on the test set: 27.60%\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca08593-132f-4efe-b022-934dc1449eef",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This tutorial provides a basic implementation of contrastive learning with SimCLR. You can further experiment by adjusting hyperparameters, using different datasets, or exploring advanced techniques like data augmentations and different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55342e07-bbbd-4515-817e-77f037dc8d98",
   "metadata": {},
   "source": [
    "**[Go to Top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
