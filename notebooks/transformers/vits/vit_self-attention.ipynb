{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Self-Attention Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Self-Attention Overview](#self_attention_overview)\n",
    "    * [Process](#process)\n",
    "    * [Pseudocode](#pseudocode)\n",
    "* [Self-Attention Pipeline](#self_attention_pipeline)\n",
    "    * [Imports](#imports)\n",
    "    * [Mini-Example](#mini)\n",
    "    * [Random Image](#image)\n",
    "    * [Image with Patches](#patches)\n",
    "    * [Model](#model)\n",
    "    * [Helper Functions](#functions)\n",
    "* [Attention Runs - Similar](#runs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Overview<a class=\"anchor\" id=\"self_attention_overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention has ascended to the forefront of deep learning, demonstrating remarkable effectiveness in a multitude of domains, encompassing image processing as well. This innovative approach has revolutionized the field of computer vision, particularly with its integration into Vision Transformers (ViTs). ViTs leverage self-attention mechanisms either as a complete substitute for, or as a collaborative partner with, traditional convolutional layers. However, it's essential to acknowledge the trade-offs inherent to this technology. While self-attention boasts numerous advantages, it often necessitates a significant increase in the number of parameters and computational resources required, compared to its established counterpart, the Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process <a class=\"anchor\" id=\"process\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Define parameters and initialize data:**\n",
    "\n",
    "- **Image size, number of images, and patch size:** These define the input image and how it's divided into smaller patches.\n",
    "- **Number of channels, heads, and dimensions:** These determine the complexity of the attention mechanism and the size of internal representations.\n",
    "\n",
    "**2. Preprocess data:**\n",
    "\n",
    "- **Extract patches:** The image is divided into overlapping or non-overlapping patches based on the defined patch size.\n",
    "- **Add positional encoding:** This step incorporates the position information of each patch into its representation, important for capturing long-range dependencies in sequences.\n",
    "\n",
    "**3. Reshape and linear transformation:**\n",
    "\n",
    "- **Reshape patches:** The patches are reshaped to a format suitable for further processing.\n",
    "- **Linear transformation:** A linear layer transforms the patch representations into three sets of vectors: query (Q), key (K), and value (V). These vectors will be used to calculate attention scores.\n",
    "\n",
    "**4. Calculate attention scores:**\n",
    "\n",
    "- **Scaled dot product:** The dot product between Q and the transpose of K is calculated and scaled by the square root of dimension for stability.\n",
    "- **Interpretation:** Higher scores indicate a stronger relationship between a specific patch (represented by Q) and another patch (represented by the corresponding row in K).\n",
    "\n",
    "**5. Apply softmax and weighted sum:**\n",
    "\n",
    "- **Softmax:** The attention scores are normalized using softmax, resulting in attention weights between 0 and 1.\n",
    "- **Weighted sum:** The value vectors (V) are weighted by the attention weights, effectively emphasizing patches relevant to the current focus based on the query patch.\n",
    "- **Interpretation:** The final output is a weighted combination of the value vectors, capturing the most relevant information from surrounding patches based on the query patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode <a class=\"anchor\" id=\"pseudocode\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pseudocode provides a high-level overview of the logic behind self-attention, without focusing on specific programming language syntax. Remember, this is a simplified representation, and the actual implementation can vary depending on the chosen framework and specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Define parameters and functions \n",
    "\n",
    "FUNCTION extract_patches(image, patch_size):\n",
    "  # Divides image into patches and returns a tensor of patches\n",
    "\n",
    "FUNCTION positional_encoding(patches):\n",
    "  # Adds positional information to each patch and returns encoded patches\n",
    "\n",
    "2. Preprocess data\n",
    "\n",
    "patches = extract_patches(image, patch_size)\n",
    "patches_with_pos = positional_encoding(patches.flatten())\n",
    "\n",
    "3. Reshape and linear transformation\n",
    "\n",
    "num_patches = patches_with_pos.shape[0]\n",
    "patches_with_pos_reshaped = patches_with_pos.reshape(num_patches, -1)  # Flatten for linear layer\n",
    "\n",
    "FUNCTION linear_qkv(x):\n",
    "  # Linear transformation to generate Q, K, V vectors\n",
    "  return Q, K, V\n",
    "\n",
    "Q, K, V = linear_qkv(patches_with_pos_reshaped)\n",
    "\n",
    "4. Calculate attention scores\n",
    "\n",
    "d_k = dim // num_heads  # Dimension of Q, K, and V after splitting\n",
    "scale = 1 / math.sqrt(d_k)  # Scaling factor\n",
    "\n",
    "attention_scores = Q.dot(K.transpose()) * scale\n",
    "\n",
    "5. Apply softmax and weighted sum\n",
    "\n",
    "attention_weights = softmax(attention_scores, axis=-1)  # Softmax along the last dimension\n",
    "output = attention_weights.dot(V)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Pipeline <a class=\"anchor\" id=\"self_attention_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version is: 2.2.0a0+6a974be\n",
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Union\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models as torchvision_models\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "from ai_tutorials import vision_transformer as vits\n",
    "\n",
    "print(f'Torch version is: {torch.__version__}')\n",
    "print(f'Is CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Example <a class=\"anchor\" id=\"mini\"></a>\n",
    "\n",
    "Briefly introduce the concept of self-attention and its role in tasks like machine translation or sentiment analysis.\n",
    "Mention that this code snippet demonstrates a simplified example of a single step within a self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Concatenated Embedding:\n",
      "[[ 1  2  3  7  8]\n",
      " [ 4  5  6  9 10]]\n",
      "Shape of concatenated embedding: (2, 5)\n",
      "Embedding dimension: 5\n",
      "Total embedding dimension (d_model): 5\n",
      "Dimension of keys and values (d_k and d_v): 2\n",
      "-----\n",
      "Query, Key, and Value Matrices:\n",
      "Q: [[18.3785915  12.9155287 ]\n",
      " [29.45144842 19.88750162]]\n",
      "K: [[5.01985598 3.13856459]\n",
      " [9.50817132 6.72543567]]\n",
      "V: [[ 5.59360392  9.77897866]\n",
      " [10.96456081 15.34470326]]\n",
      "-----\n",
      "Attention Scores:\n",
      "Scores: [[ 93.89961106 184.98574822]\n",
      " [148.67643998 292.58772576]]\n",
      "-----\n",
      "Attention Weights:\n",
      "Weights: [[2.76562476e-40 1.00000000e+00]\n",
      " [3.16317123e-63 1.00000000e+00]]\n",
      "-----\n",
      "Output after Weighted Sum - Self-Attention:\n",
      "Shape of output: (2, 2)\n",
      "[[10.96456081 15.34470326]\n",
      " [10.96456081 15.34470326]]\n"
     ]
    }
   ],
   "source": [
    "# The softmax function is used to convert scores into probabilities, \n",
    "# ensuring they sum to 1. I recommend using nn.functional.\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "# Example concatenated embedding\n",
    "concatenated_embedding = np.array([[1, 2, 3, 7, 8],\n",
    "                                   [4, 5, 6, 9, 10]])\n",
    "\n",
    "print(\"Example Concatenated Embedding:\")\n",
    "print(concatenated_embedding)\n",
    "print(f\"Shape of concatenated embedding: {concatenated_embedding.shape}\")\n",
    "print(f\"Embedding dimension: {concatenated_embedding.shape[-1]}\")\n",
    "\n",
    "# Assuming the number of attention heads and other parameters\n",
    "num_heads = 2\n",
    "d_model = concatenated_embedding.shape[-1]  # Dimension of the concatenated embedding\n",
    "print(f\"Total embedding dimension (d_model): {d_model}\")\n",
    "d_k = d_v = d_model // num_heads\n",
    "print(f\"Dimension of keys and values (d_k and d_v): {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Generate Query, Key, and Value matrices\n",
    "Q = concatenated_embedding.dot(np.random.rand(d_model, d_k))  # Query matrix\n",
    "K = concatenated_embedding.dot(np.random.rand(d_model, d_k))  # Key matrix\n",
    "V = concatenated_embedding.dot(np.random.rand(d_model, d_v))  # Value matrix\n",
    "print(\"Query, Key, and Value Matrices:\")\n",
    "print(f\"Q: {Q}\\nK: {K}\\nV: {V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "print(\"Attention Scores:\")\n",
    "print(f\"Scores: {attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = softmax(attention_scores, axis=-1)\n",
    "print(\"Attention Weights:\")\n",
    "print(f\"Weights: {attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V)\n",
    "print(\"Output after Weighted Sum - Self-Attention:\")\n",
    "print(f\"Shape of output: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64x64 Random Images and Positional Encoding <a class=\"anchor\" id=\"image\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embedding):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    print(f\"Embedding Dimension: {embedding.shape}\")\n",
    "    seq_len = embedding.shape[1]\n",
    "    channel_dim = embedding.shape[-1] # channels\n",
    "    print(\"Seq len:\", seq_len)\n",
    "    print(\"Channel dim:\", channel_dim)\n",
    "    print(f\"Np zeros shape: {np.zeros((seq_len, channel_dim)).shape}\")\n",
    "#     print(f\"Np zeros: {np.zeros((seq_len, channel_dim))}\")\n",
    "    pos_enc = np.zeros((seq_len, channel_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, channel_dim, 2):\n",
    "            pos_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) // channel_dim)))\n",
    "            if i + 1 < channel_dim:  # Ensure not to exceed the last dimension\n",
    "                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) // channel_dim)))\n",
    "    return embedding + pos_enc\n",
    "\n",
    "image_size = 64 # High image size will not work on small GPU\n",
    "num_images = 3\n",
    "\n",
    "# Parameters\n",
    "num_channels = 3  # 3 channels for RGB images\n",
    "num_heads = 2\n",
    "dim = image_size * image_size * num_channels  # Dimension of the input (total number of features)\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "images = np.random.rand(num_images, image_size, image_size, num_channels)\n",
    "print(f\"IMAGES SHAPE: {images.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Reshape images to have a sequence length of image_size * image_size\n",
    "images_flattened = images.reshape(num_images, -1, num_channels)\n",
    "print(f\"FLATTENED SHAPE: {images_flattened.shape}\")\n",
    "print(f\"FLATTENED: {images_flattened}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Add positional encoding\n",
    "print(f\"POSITIONAL ENCODING OUTPUT (USING FLATTENED IMAGE)\")\n",
    "images_with_pos = positional_encoding(images_flattened)\n",
    "print(f\"Input images after positional encoding: (shape={images_with_pos.shape}\")\n",
    "print(images_with_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Basic np.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images_with_pos to align with the matrix multiplication\n",
    "images_with_pos_reshaped = images_with_pos.reshape(num_images, -1, dim)\n",
    "\n",
    "# Generate Query, Key, and Value matrices\n",
    "Q = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_k))  # Query matrix\n",
    "K = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_k))  # Key matrix\n",
    "V = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_v))  # Value matrix\n",
    "print(f\"Q: {Q}\\nK: {K}\\nV: {V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = None or d_k**-0.5\n",
    "\n",
    "attention_scores = np.matmul(Q, K.transpose((0, 2, 1))) / np.sqrt(d_k) # Standard\n",
    "print(f\"scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores, axis=-1)\n",
    "print(f\"weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V)\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(f\"OUTPUT:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add positional encoding\n",
    "images_with_pos = positional_encoding(images_flattened)\n",
    "print(\"Input images after positional encoding:\")\n",
    "print(images_with_pos)\n",
    "print(f\"IMAGES WITH POS SHAPE: {images_with_pos.shape}\")\n",
    "\n",
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(num_channels, d_k * 3, bias=False) # Linear(in_features=3, out_features=18432, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(images_with_pos, dtype=torch.float32))\n",
    "print(f\"QKV AFTER LINEAR: {qkv.shape}\")\n",
    "qkv = qkv.reshape(num_images, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "print(f\"QKV AFTER RESHAPE: {qkv.shape}\")\n",
    "\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores.detach().numpy(), axis=-1)\n",
    "print(f\"weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V.detach().numpy())\n",
    "\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image with patches example <a class=\"anchor\" id=\"patches\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embedding):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    seq_len = embedding.shape[1]\n",
    "    channel_dim = embedding.shape[-1]\n",
    "    pos_enc = np.zeros((seq_len, channel_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, channel_dim, 2):\n",
    "            pos_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) // channel_dim)))\n",
    "            if i + 1 < channel_dim:  \n",
    "                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) // channel_dim)))\n",
    "    return embedding + pos_enc\n",
    "\n",
    "def extract_patches(images, patch_size):\n",
    "    patches = []\n",
    "    for image in images:\n",
    "        for i in range(0, image.shape[0] - patch_size + 1, patch_size):\n",
    "            for j in range(0, image.shape[1] - patch_size + 1, patch_size):\n",
    "                patch = image[i:i+patch_size, j:j+patch_size, :]\n",
    "                patches.append(patch.flatten())\n",
    "    return np.array(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example images (for demonstration, you would use your actual images here)\n",
    "image_w = 500\n",
    "image_h = 375\n",
    "\n",
    "# Parameters\n",
    "num_channels = 3 # Assuming RGB images\n",
    "num_heads = 8\n",
    "patch_size = 16 # Patch size (similar to DINO)\n",
    "dim = num_channels * patch_size * patch_size  # Dimension of each patch\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "image = np.random.rand(1, image_h, image_w, num_channels)[0]\n",
    "print(f\"IMAGE SHAPE: {image.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Extract patches from the input image\n",
    "image_patches = extract_patches([image], patch_size)  # Pass a list containing the single image\n",
    "print(f\"PATCHES SHAPE: {image_patches.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply positional encoding to each patch individually\n",
    "num_patches = image_patches.shape[0]\n",
    "pos_enc = positional_encoding(image_patches.reshape(num_patches, -1, num_channels))\n",
    "print(\"Positional encoding:\")\n",
    "print(f\"SHAPE: {pos_enc.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Concat positional encoding with image_patches\n",
    "image_patches_with_pos = pos_enc.reshape(1, -1, dim)\n",
    "print(\"Image patches after positional encoding:\")\n",
    "print(f\"SHAPE: {image_patches_with_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "print(linear_qkv)\n",
    "\n",
    "# Apply linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(image_patches_with_pos, dtype=torch.float32))\n",
    "qkv = qkv.reshape(1, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "# Transpose K before performing matrix multiplication\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"Attention scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "print(f\"Attention weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = torch.matmul(attention_weights, V)\n",
    "print(f\"Output after self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (500, 375)\n",
    "num_images = 1\n",
    "patch_size = 16  # Define the patch size\n",
    "\n",
    "num_channels = 3\n",
    "num_heads = 2\n",
    "dim = patch_size * patch_size * num_channels  # Dimension of each patch\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "images = np.random.rand(num_images, image_size[1], image_size[0], num_channels)  # Note: OpenCV style (width, height)\n",
    "print(f\"IMAGE SHAPE: {images.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Extract patches from images\n",
    "patches = extract_patches(images, patch_size)\n",
    "print(f\"PATCHES SHAPE: {patches.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Add positional encoding\n",
    "num_patches = patches.shape[0]\n",
    "patches_with_pos = positional_encoding(patches.reshape(num_patches, -1, num_channels))\n",
    "print(f\"PATCHES WITH POS: {patches_with_pos.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Reshape patches_with_pos to match the expected input shape for linear layer\n",
    "patches_with_pos_reshaped = patches_with_pos.reshape(num_images, -1, dim)\n",
    "print(f\"PATCHES WITH POS RESHAPE: {patches_with_pos.shape}\")\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(patches_with_pos_reshaped, dtype=torch.float32))\n",
    "qkv = qkv.reshape(num_images, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"Attention scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "print(f\"Attention weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = torch.matmul(attention_weights, V)\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"vit_small\" # num_heads = 6\n",
    "checkpoint_key = \"teacher\"\n",
    "patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arch in torchvision_models.__dict__.keys():\n",
    "    model = (torchvision_models.__dict__[arch](\n",
    "        num_classes=0))\n",
    "    model.fc = nn.Identity()\n",
    "else:\n",
    "    model = vits.__dict__[arch](\n",
    "        patch_size=patch_size, num_classes=0)\n",
    "\n",
    "url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "\n",
    "state_dict = torch.hub.load_state_dict_from_url(\n",
    "    url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions <a class=\"anchor\" id=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = pth_transforms.Compose([\n",
    "    # pth_transforms.Resize((480, 480)),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(img_name, path, patch_size):\n",
    "    img_path = f\"{path}/{img_name}\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    print(f\"Original Image Size: {img.size}\")\n",
    "\n",
    "    img = transform(img)\n",
    "    print(f\"Transformed Image Size: {img.shape}\")\n",
    "        \n",
    "    # Make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "    print(f\"Image by Patch Size: {img.shape}\")\n",
    "    \n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    y, attentions, test_scores, test_weights = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1]  # Number of heads\n",
    "\n",
    "    # Keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    \n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "    \n",
    "    return y, attentions, test_scores, test_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Attentions Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img, img_per_col = 3, ):\n",
    "    num_images = len(img)\n",
    "    num_images\n",
    "\n",
    "    nrow = num_images // img_per_col\n",
    "    ncol = num_images // nrow\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrow, ncol,\n",
    "        gridspec_kw=dict(wspace=0.05, hspace=0.05,\n",
    "                         top=1. - 0.5 / (nrow + 1), bottom=0.5 / (nrow + 1),\n",
    "                         left=0.5 / (ncol + 1), right=1 - 0.5 / (ncol + 1)),\n",
    "        figsize=(ncol + 10, nrow + 10)\n",
    "    )\n",
    "\n",
    "    for i, image in enumerate(img):\n",
    "        ax = axes[i // ncol, i % ncol]\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Run <a class=\"anchor\" id=\"runs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = \"/usr/src/ai_tutorials/data/coco_2017/val2017\"\n",
    "img_name = \"000000000139.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(full_path + '/' + img_name)\n",
    "print(f\"IMG SHAPE: {img.size}\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run attention pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, img_attentions, scores, weights = get_attention(img_name=img_name, path=full_path, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_attentions), img_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION SCORES: torch.Size([1, 6, 1041, 1041])\n",
    "# tensor([[ 3.4057, -3.8283, -3.6789, -3.8322, -4.2521],\n",
    "#         [ 2.5470, -2.0419, -1.8337, -2.0044, -2.2881],\n",
    "#         [ 2.5867, -1.9636, -1.7265, -1.9115, -2.1393],\n",
    "#         [ 2.4455, -1.8812, -1.6428, -1.8065, -2.0402],\n",
    "#         [ 2.4473, -1.8908, -1.6959, -1.8473, -2.0370]], device='cuda:0')\n",
    "\n",
    "# ATTENTION WEIGHTS (SOFTMAX): torch.Size([1, 6, 1041, 1041])\n",
    "# tensor([[5.1433e-02, 3.7115e-05, 4.3097e-05, 3.6970e-05, 2.4294e-05],\n",
    "#         [2.3387e-02, 2.3772e-04, 2.9273e-04, 2.4679e-04, 1.8583e-04],\n",
    "#         [2.3418e-02, 2.4740e-04, 3.1357e-04, 2.6064e-04, 2.0753e-04],\n",
    "#         [2.0375e-02, 2.6918e-04, 3.4163e-04, 2.9003e-04, 2.2961e-04],\n",
    "#         [2.0554e-02, 2.6846e-04, 3.2624e-04, 2.8040e-04, 2.3195e-04]],\n",
    "#        device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_score = scores[0][0][:5][:5]\n",
    "first_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute softmax along the last dimension (dim=1)\n",
    "attn_weights = first_score.softmax(dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_weight = weights[0][0][:5][:5]\n",
    "first_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img=img_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
