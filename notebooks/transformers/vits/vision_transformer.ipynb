{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a74e41d",
   "metadata": {},
   "source": [
    "# Vision Transformer (Incomplete) <a class=\"anchor\" id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ecdd40",
   "metadata": {},
   "source": [
    "Vision Transformers (ViT), since their introduction by Dosovitskiy et. al. [reference] in 2020, have dominated the field of Computer Vision.\n",
    "\n",
    "In a vision transformer, an input image is divided into smaller patches, similar to how a CNN processes local image regions. These patches are then flattened and fed into the transformer architecture. The transformer comprises multiple layers of self-attention and feed-forward neural networks, allowing it to learn both local and global relationships between patches.\n",
    "\n",
    "The self-attention mechanism enables the model to attend to different patches and learn their relationships, which helps in capturing long-range dependencies in images. Additionally, vision transformers can be pretrained on large datasets, such as ImageNet, and then fine-tuned on specific tasks.\n",
    "\n",
    "This tutorial will provide a basic vision transformer from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384cb73",
   "metadata": {},
   "source": [
    "### Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0d103",
   "metadata": {},
   "source": [
    "[**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**](https://arxiv.org/abs/2010.11929).<br />\n",
    "Github: [**Google Research - Vision Transformer**](https://github.com/google-research/vision_transformer)\n",
    "\n",
    "[**Attention is all you need**](https://arxiv.org/pdf/1706.03762.pdf).<br />\n",
    "Github: [**Natural Language Processing Lab**](https://github.com/jadore801120/attention-is-all-you-need-pytorch)(not official but quite good!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2e1c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Overview](#overview)\n",
    "    * [Training Process](#training_process)\n",
    "* [Building Models](#models)\n",
    "    * [Imports](#imports)\n",
    "    * [Architectures](#arch)\n",
    "* [FAQ](#faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b803d",
   "metadata": {},
   "source": [
    "# Overview<a class=\"anchor\" id=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48fa40",
   "metadata": {},
   "source": [
    "Transformer models revolutionized Natural Language Processing (NLP). They have become a de-facto standard for modern NLP tasks and display obvious performance boost when compared to models like LSTMs and GRUs.\n",
    "\n",
    "The most important paper that transformed the NLP landscape is the \"[**Attention is all you need**](https://arxiv.org/pdf/1706.03762.pdf)\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909e423",
   "metadata": {},
   "source": [
    "![Transformer Architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910cf42",
   "metadata": {},
   "source": [
    "## Training Process<a class=\"anchor\" id=\"training_process\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4d85a",
   "metadata": {},
   "source": [
    "**Data Preparation**:\n",
    "- Collect and preprocess your training data. This often includes resizing images to a consistent size, data augmentation, and splitting the dataset into training, validation, and test sets.\n",
    "- Organize the data with appropriate labels, especially if you are working on a supervised task like image classification or object detection.\n",
    "\n",
    "**Model Architecture**:\n",
    "- Define the architecture of your ViT. This includes specifying the number of layers, patch size, embedding dimensions, number of attention heads, and the structure of the feed-forward networks.\n",
    "- Pre-trained models can also be used as a starting point, fine-tuned for your specific task.\n",
    "\n",
    "**Data Encoding**:\n",
    "- Convert your image data into a format that the ViT can understand. This usually involves splitting images into non-overlapping patches and linearly projecting these patches into embedding vectors.\n",
    "\n",
    "**Positional Encoding**:\n",
    "- Since ViTs don't have built-in spatial information like convolutional networks, you need to add positional encodings to the patch embeddings. This informs the model about the relative positions of patches.\n",
    "\n",
    "**Loss Function**:\n",
    "- Define an appropriate loss function for your task. For image classification, cross-entropy loss is commonly used. For object detection, you may use a combination of localization and classification losses.\n",
    "\n",
    "**Training Objective**:\n",
    "- ViTs can be pre-trained on large datasets (pre-training) and fine-tuned on your specific task (fine-tuning). The pre-training stage often involves tasks like image classification or predicting patch permutations.\n",
    "\n",
    "**Training**:\n",
    "- Train the ViT on your task-specific dataset using the defined loss function.\n",
    "- Backpropagate gradients and update the model's parameters using optimization techniques like Adam or SGD.\n",
    "- Monitor training progress with validation data and consider early stopping to avoid overfitting.\n",
    "\n",
    "**Evaluation**:\n",
    "- After training, evaluate your ViT on a held-out test dataset to assess its performance. Common evaluation metrics include accuracy, mAP (mean Average Precision), or other task-specific metrics.\n",
    "\n",
    "**Hyperparameter Tuning**:\n",
    "- Fine-tune hyperparameters like learning rate, batch size, and architectural choices to optimize the model's performance.\n",
    "\n",
    "**Deployment**:\n",
    "- Once your ViT is trained and evaluated, you can deploy it to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245d174",
   "metadata": {},
   "source": [
    "# Building Models<a class=\"anchor\" id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae2868",
   "metadata": {},
   "source": [
    "## IMPORTS<a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6bde7f6f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4b061068f0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f11577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ab4c0",
   "metadata": {},
   "source": [
    "## Architectures<a class=\"anchor\" id=\"arch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cdf39",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH\n",
    "IMAGE_CHANNELS = 3\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
    "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5a84f",
   "metadata": {},
   "source": [
    "### Image from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\n",
    "req = urlopen(url)\n",
    "arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "img = cv2.imdecode(arr, -1) # 'Load it as it is'\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785cea6",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "310d2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_transform using Compose\n",
    "transform_img = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((224,224))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe6530",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "### Image Patching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e592554",
   "metadata": {},
   "source": [
    "We start with splitting the inpute image into sub-images of equal sizes - patches. Each of the sub-images/patchs goes through a linear embedding resulting in a 1-d vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "38854b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        assert len(x) == 2\n",
    "        return x\n",
    "\n",
    "    assert isinstance(x, int)\n",
    "    return (x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e0334890",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def patching(\n",
    "    img,\n",
    "    embed_dim = 768,\n",
    "    flatten_embedding = True,\n",
    "    img_size = 224,\n",
    "    img_chans = 3,\n",
    "    norm_layer = None,\n",
    "    patch_size = 16\n",
    "):\n",
    "    image_HW = make_2tuple(img_size) # img_size, img_size\n",
    "    patch_HW = make_2tuple(patch_size) # patch_size, patch_size\n",
    "\n",
    "    img = transform_img(img) # torch.Size([3, 224, 224])\n",
    "    img = img.unsqueeze(0) # torch.Size([1, 3, 224, 224])\n",
    "    \n",
    "    _, _, H, W = img.shape\n",
    "    img_size = image_HW\n",
    "    patch_size = patch_HW\n",
    "    \n",
    "    patch_H, patch_W = patch_size\n",
    "    \n",
    "    assert W % patch_W == 0 and H % patch_H == 0, \\\n",
    "        print(\"Image Width is not divisible by patch size\")\n",
    "    \n",
    "    proj = nn.Conv2d(img_chans,\n",
    "                     embed_dim,\n",
    "                     kernel_size=patch_HW,\n",
    "                     stride=patch_HW)\n",
    "    x = proj(img) # Batch (B), Channels (C), Height (H), Width (W)\n",
    "    H, W = x.size(2), x.size(3)\n",
    "#     x = x.flatten(2).transpose(1,2) # B HW C\n",
    "    x = torch.einsum('ijk -> ijk', x.flatten(2)) # same as transpose\n",
    "#     print(torch.all(x.eq(x2)))\n",
    "    \n",
    "    norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "    x = norm(x)\n",
    "    if not flatten_embedding:\n",
    "        x = x.reshape(-1, H, W, embed_dim) # B, H, W, C\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0804ded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1330,  0.1418,  0.1928,  ...,  0.1036, -0.0242,  0.2727],\n",
       "         [ 0.5675,  0.5009,  0.4831,  ...,  0.2510,  0.0497,  0.1118],\n",
       "         [ 0.1200,  0.1250,  0.1544,  ...,  0.0028,  0.0744,  0.0110],\n",
       "         ...,\n",
       "         [ 0.6000,  0.6404,  0.6084,  ...,  0.2332,  0.1651,  0.1323],\n",
       "         [ 0.1264,  0.1149,  0.0716,  ...,  0.0388,  0.0849, -0.0154],\n",
       "         [-0.2970, -0.2613, -0.2429,  ..., -0.1754, -0.0784,  0.1188]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patching(img = test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218024a3",
   "metadata": {},
   "source": [
    "### Classification token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3ecbd",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe0bf8",
   "metadata": {},
   "source": [
    "### Endcoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b6352",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1f1c5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image, n_patches=16):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.image = image\n",
    "        self.chw = image.shape # (H, W, C)\n",
    "        print(self.chw)\n",
    "        self.n_patches = n_patches\n",
    "        \n",
    "    def forward(self, img = None):\n",
    "        img = img if img is not None else self.image\n",
    "        patches = patching(img = img)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538e98d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e8cbd06c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2848, 2136, 3)\n"
     ]
    }
   ],
   "source": [
    "vit_test = VisionTransformer(image = test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2f924676",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = vit_test.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3506ea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2537, -0.2921, -0.3430,  ..., -0.0436, -0.1249, -0.0270],\n",
       "         [ 0.2203,  0.2702,  0.3274,  ...,  0.0705,  0.1268,  0.1792],\n",
       "         [-0.3846, -0.4100, -0.4190,  ..., -0.1242, -0.1103, -0.1355],\n",
       "         ...,\n",
       "         [ 0.0136,  0.0054, -0.0144,  ...,  0.1597,  0.0722, -0.0587],\n",
       "         [ 0.5060,  0.5510,  0.5870,  ...,  0.3285,  0.4363,  0.2380],\n",
       "         [ 0.3207,  0.3567,  0.3643,  ...,  0.0378,  0.2446, -0.0531]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b3a5a",
   "metadata": {},
   "source": [
    "# Notes<a class=\"anchor\" id=\"faq\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75daa23e",
   "metadata": {},
   "source": [
    "**CNNs**:\n",
    "- Philosophy:\n",
    "    - Pixels are dependent on their neighboring pixels. Important features and edges are extracted using filters on a patch of an image.\n",
    "- Advantage:\n",
    "    - Perform better with a smaller labeled dataset\n",
    "    - Compact and efficient memory utilization\n",
    "- Disadvantage:\n",
    "    - Does not provide details of each pixel of an image\n",
    "    - Convolving can lead to bias (inductive)\n",
    "\n",
    "**ViTs**:\n",
    "- Philosophy:\n",
    "    - Instead of parts that the filters can extract, feed a model with entire image data.\n",
    "- Advantage:\n",
    "    - Labeled data aren't necessary needed\n",
    "    - Generalize better\n",
    "- Disadvantage:\n",
    "    - Require a lot of data to be effective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307ff1d",
   "metadata": {},
   "source": [
    "---\n",
    "[**Back to top**](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
