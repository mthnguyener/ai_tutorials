{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "* [Self-Attention Overview](#self_attention_overview)\n",
    "    * [Process](#process)\n",
    "    * [Pseudocode](#pseudocode)\n",
    "* [Self-Attention Pipeline](#self_attention_pipeline)\n",
    "    * [Imports](#imports)\n",
    "    * [Mini-Example](#mini)\n",
    "    * [Random Image](#image)\n",
    "    * [Image with Patches](#patches)\n",
    "    * [Model](#model)\n",
    "    * [Helper Functions](#functions)\n",
    "* [Attention Runs - Similar](#runs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Overview<a class=\"anchor\" id=\"self_attention_overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention has ascended to the forefront of deep learning, demonstrating remarkable effectiveness in a multitude of domains, encompassing image processing as well. This innovative approach has revolutionized the field of computer vision, particularly with its integration into Vision Transformers (ViTs). ViTs leverage self-attention mechanisms either as a complete substitute for, or as a collaborative partner with, traditional convolutional layers. However, it's essential to acknowledge the trade-offs inherent to this technology. While self-attention boasts numerous advantages, it often necessitates a significant increase in the number of parameters and computational resources required, compared to its established counterpart, the Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process <a class=\"anchor\" id=\"process\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Feature Extraction: The Foundation**\n",
    "\n",
    "The process commences with feeding the image into a pre-trained convolutional neural network (CNN) akin to traditional approaches. This CNN acts as a feature extractor, meticulously dissecting the image and capturing low-level details like edges, textures, and color variations. These extracted details, represented as feature maps, provide the essential groundwork upon which self-attention builds its understanding.\n",
    "\n",
    "2. **Unveiling the Power of Three: Queries, Keys, and Values**\n",
    "\n",
    "Each element within the feature maps is then transformed into a trio of novel vectors, each playing a distinct role in the self-attention mechanism:\n",
    "\n",
    "Query (Q): This vector embodies the essence of what the model seeks within the intricate tapestry of the feature maps. Imagine it as a specific question the model poses about each region of the image, aiming to unearth the most pertinent information.\n",
    "\n",
    "Key (K): In contrast to the inquisitive nature of the query vector, the key vector acts as a repository of knowledge about each element in the feature maps. It's akin to an answer key, holding the potential answers that correspond to the queries posed by the model.\n",
    "\n",
    "Value (V): This vector carries the actual content or information associated with each element within the feature maps. It's the treasure trove of data that the model will selectively pay attention to, based on the efficacy of the query-key matching process.\n",
    "\n",
    "3. **The Heart of the Mechanism: Attention Scores**\n",
    "\n",
    "The crux of self-attention lies in calculating a score for every conceivable relationship between elements in the feature maps. This score meticulously quantifies how relevant a particular element's value is in response to the current query, considering the corresponding key as the intermediary. Mathematically, the similarity between the query and key vectors is often computed using a dot product operation. Intuitively, a higher dot product signifies a stronger alignment between the query and the key, implying greater relevance of the associated value.\n",
    "\n",
    "4. **A Dance of Weights: Assigning Importance**\n",
    "\n",
    "Once the attention scores for all potential relationships are meticulously calculated, the model embarks on the task of assigning weights to each value vector. These weights act as a measure of significance, indicating how important the information encoded within each element is relative to the current query. Elements that generate higher attention scores naturally translate to higher weights. In essence, the model is meticulously prioritizing its focus on the most relevant features within the image for the specific task at hand.\n",
    "\n",
    "5. **The Weighted Sum: Aggregating Knowledge**\n",
    "\n",
    "Finally, the model takes a weighted sum of all the value vectors, leveraging the meticulously calculated attention weights. This process effectively creates a refined representation for the current element, emphasizing the features that hold the most relevance for the specific query. Imagine the model intelligently amplifying the most critical details while judiciously suppressing less important ones.\n",
    "\n",
    "6. **A Meticulous Journey: Repetition and Refinement**\n",
    "\n",
    "This entire intricate dance of transformations, attention score calculations, weight assignment, and weighted summation (steps 2-5) is meticulously repeated for each and every element within the feature maps. Essentially, the model progresses through the image piece by piece, attentively gleaning the most crucial information for the task at hand. It's akin to the model meticulously examining each brushstroke in a painting, meticulously piecing together the narrative it conveys.\n",
    "\n",
    "**Beyond the Basics: Multi-Head Attention**\n",
    "\n",
    "While the explanation above delves into the core workings of self-attention, there are further advancements to explore. One such technique is multi-head attention, which involves performing multiple self-attention operations in parallel. Each of these \"heads\" can attend to different aspects of the relationships within the image, effectively capturing a richer and more nuanced understanding of the intricate interplay between image elements. This allows the model to not only identify the most relevant features but also to grasp the subtle interplay between them, leading to superior performance in computer vision tasks.\n",
    "\n",
    "By understanding the intricate workings of self-attention mechanisms, you gain valuable insights into how deep learning models are revolutionizing the field of image processing. This powerful technique allows models to not only focus on individual features but also to grasp the crucial relationships that tie an image together, leading to significant advancements in tasks like object recognition, image segmentation, and image captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode <a class=\"anchor\" id=\"pseudocode\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pseudocode provides a high-level overview of the logic behind self-attention, without focusing on specific programming language syntax. Remember, this is a simplified representation, and the actual implementation can vary depending on the chosen framework and specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Input: data - a 2D array representing the feature map\n",
    "Output: output - a 2D array representing the attention-weighted output\n",
    "\n",
    "# Function: self_attention(data)\n",
    "function self_attention(data):\n",
    "  # Get dimensions of the data\n",
    "  num_elements, feature_dim = data.shape\n",
    "\n",
    "  # Initialize empty arrays for query, key, value (usually learned during training)\n",
    "  query = new array(num_elements, feature_dim)\n",
    "  key = new array(num_elements, feature_dim)\n",
    "  value = new array(num_elements, feature_dim)\n",
    "\n",
    "  # (Fill query, key, and value with actual values, typically learned parameters)\n",
    "\n",
    "  # Calculate attention scores using dot product (query and transposed key)\n",
    "  attention_scores = dot_product(query, transpose(key))\n",
    "\n",
    "  # Apply softmax for normalized weights\n",
    "  attention_weights = softmax(attention_scores)\n",
    "\n",
    "  # Initialize empty array for output\n",
    "  output = new array(num_elements, feature_dim)\n",
    "\n",
    "  # Loop through each element in the data\n",
    "  for i in range(num_elements):\n",
    "    # Weighted sum of value vectors using attention weights\n",
    "    for j in range(feature_dim):\n",
    "      output[i, j] = 0\n",
    "      for k in range(num_elements):\n",
    "        output[i, j] += attention_weights[i, k] * value[k, j]\n",
    "\n",
    "  # Return the attention-weighted output\n",
    "  return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Pipeline <a class=\"anchor\" id=\"self_attention_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Union\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models as torchvision_models\n",
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "from notebooks.transformers import vision_transformer as vits\n",
    "\n",
    "print(f'Torch version is: {torch.__version__}')\n",
    "print(f'Is CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Example <a class=\"anchor\" id=\"mini\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "# Example concatenated embedding\n",
    "concatenated_embedding = np.array([[1, 2, 3, 7, 8],\n",
    "                                   [4, 5, 6, 9, 10]])\n",
    "print(f\"shape: {concatenated_embedding.shape}\")\n",
    "print(f\"shape -1: {concatenated_embedding.shape[-1]}\")\n",
    "\n",
    "# Assuming the number of attention heads and other parameters\n",
    "num_heads = 2\n",
    "d_model = concatenated_embedding.shape[-1]  # Dimension of the concatenated embedding\n",
    "print(f\"d_model: {d_model}\")\n",
    "d_k = d_v = d_model // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Generate Query, Key, and Value matrices\n",
    "Q = concatenated_embedding.dot(np.random.rand(d_model, d_k))  # Query matrix\n",
    "K = concatenated_embedding.dot(np.random.rand(d_model, d_k))  # Key matrix\n",
    "V = concatenated_embedding.dot(np.random.rand(d_model, d_v))  # Value matrix\n",
    "print(f\"Q: {Q}\\nK: {K}\\nV: {V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "print(f\"scores: {attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = softmax(attention_scores, axis=-1)\n",
    "print(f\"weights: {attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V)\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64x64 Random Images and Positional Encoding <a class=\"anchor\" id=\"image\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embedding):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    print(f\"Embedding Dimension: {embedding.shape}\")\n",
    "    seq_len = embedding.shape[1]\n",
    "    channel_dim = embedding.shape[-1] # channels\n",
    "    print(\"Seq len:\", seq_len)\n",
    "    print(\"Channel dim:\", channel_dim)\n",
    "    print(f\"Np zeros shape: {np.zeros((seq_len, channel_dim)).shape}\")\n",
    "#     print(f\"Np zeros: {np.zeros((seq_len, channel_dim))}\")\n",
    "    pos_enc = np.zeros((seq_len, channel_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, channel_dim, 2):\n",
    "            pos_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) // channel_dim)))\n",
    "            if i + 1 < channel_dim:  # Ensure not to exceed the last dimension\n",
    "                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) // channel_dim)))\n",
    "    return embedding + pos_enc\n",
    "\n",
    "image_size = 64 # High image size will not work on small GPU\n",
    "num_images = 3\n",
    "\n",
    "# Parameters\n",
    "num_channels = 3  # 3 channels for RGB images\n",
    "num_heads = 2\n",
    "dim = image_size * image_size * num_channels  # Dimension of the input (total number of features)\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "images = np.random.rand(num_images, image_size, image_size, num_channels)\n",
    "print(f\"IMAGES SHAPE: {images.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Reshape images to have a sequence length of image_size * image_size\n",
    "images_flattened = images.reshape(num_images, -1, num_channels)\n",
    "print(f\"FLATTENED SHAPE: {images_flattened.shape}\")\n",
    "print(f\"FLATTENED: {images_flattened}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Add positional encoding\n",
    "print(f\"POSITIONAL ENCODING OUTPUT (USING FLATTENED IMAGE)\")\n",
    "images_with_pos = positional_encoding(images_flattened)\n",
    "print(f\"Input images after positional encoding: (shape={images_with_pos.shape}\")\n",
    "print(images_with_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Basic np.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images_with_pos to align with the matrix multiplication\n",
    "images_with_pos_reshaped = images_with_pos.reshape(num_images, -1, dim)\n",
    "\n",
    "# Generate Query, Key, and Value matrices\n",
    "Q = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_k))  # Query matrix\n",
    "K = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_k))  # Key matrix\n",
    "V = np.matmul(images_with_pos_reshaped, np.random.rand(dim, d_v))  # Value matrix\n",
    "print(f\"Q: {Q}\\nK: {K}\\nV: {V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = None or d_k**-0.5\n",
    "\n",
    "attention_scores = np.matmul(Q, K.transpose((0, 2, 1))) / np.sqrt(d_k) # Standard\n",
    "print(f\"scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = softmax(attention_scores, axis=-1)\n",
    "print(f\"weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V)\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(f\"OUTPUT:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add positional encoding\n",
    "images_with_pos = positional_encoding(images_flattened)\n",
    "print(\"Input images after positional encoding:\")\n",
    "print(images_with_pos)\n",
    "print(f\"IMAGES WITH POS SHAPE: {images_with_pos.shape}\")\n",
    "\n",
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(num_channels, d_k * 3, bias=False) # Linear(in_features=3, out_features=18432, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(images_with_pos, dtype=torch.float32))\n",
    "print(f\"QKV AFTER LINEAR: {qkv.shape}\")\n",
    "qkv = qkv.reshape(num_images, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "print(f\"QKV AFTER RESHAPE: {qkv.shape}\")\n",
    "\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = softmax(attention_scores.detach().numpy(), axis=-1)\n",
    "print(f\"weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "# Compute the weighted sum using attention weights\n",
    "output = np.matmul(attention_weights, V.detach().numpy())\n",
    "\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image with patches example <a class=\"anchor\" id=\"patches\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embedding):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    seq_len = embedding.shape[1]\n",
    "    channel_dim = embedding.shape[-1]\n",
    "    pos_enc = np.zeros((seq_len, channel_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, channel_dim, 2):\n",
    "            pos_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) // channel_dim)))\n",
    "            if i + 1 < channel_dim:  \n",
    "                pos_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) // channel_dim)))\n",
    "    return embedding + pos_enc\n",
    "\n",
    "def extract_patches(images, patch_size):\n",
    "    patches = []\n",
    "    for image in images:\n",
    "        for i in range(0, image.shape[0] - patch_size + 1, patch_size):\n",
    "            for j in range(0, image.shape[1] - patch_size + 1, patch_size):\n",
    "                patch = image[i:i+patch_size, j:j+patch_size, :]\n",
    "                patches.append(patch.flatten())\n",
    "    return np.array(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example images (for demonstration, you would use your actual images here)\n",
    "image_w = 500\n",
    "image_h = 375\n",
    "\n",
    "# Parameters\n",
    "num_channels = 3 # Assuming RGB images\n",
    "num_heads = 8\n",
    "patch_size = 16 # Patch size (similar to DINO)\n",
    "dim = num_channels * patch_size * patch_size  # Dimension of each patch\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "image = np.random.rand(1, image_h, image_w, num_channels)[0]\n",
    "print(f\"IMAGE SHAPE: {image.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Extract patches from the input image\n",
    "image_patches = extract_patches([image], patch_size)  # Pass a list containing the single image\n",
    "print(f\"PATCHES SHAPE: {image_patches.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply positional encoding to each patch individually\n",
    "num_patches = image_patches.shape[0]\n",
    "pos_enc = positional_encoding(image_patches.reshape(num_patches, -1, num_channels))\n",
    "print(\"Positional encoding:\")\n",
    "print(f\"SHAPE: {pos_enc.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Concat positional encoding with image_patches\n",
    "image_patches_with_pos = pos_enc.reshape(1, -1, dim)\n",
    "print(\"Image patches after positional encoding:\")\n",
    "print(f\"SHAPE: {image_patches_with_pos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "print(linear_qkv)\n",
    "\n",
    "# Apply linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(image_patches_with_pos, dtype=torch.float32))\n",
    "qkv = qkv.reshape(1, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "# Transpose K before performing matrix multiplication\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"Attention scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "print(f\"Attention weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = torch.matmul(attention_weights, V)\n",
    "print(f\"Output after self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (500, 375)\n",
    "num_images = 1\n",
    "patch_size = 16  # Define the patch size\n",
    "\n",
    "num_channels = 3\n",
    "num_heads = 2\n",
    "dim = patch_size * patch_size * num_channels  # Dimension of each patch\n",
    "print(f\"dim: {dim}\")\n",
    "d_k = d_v = dim // num_heads\n",
    "print(f\"d_k and d_v: {d_k}\")\n",
    "print(\"-----\")\n",
    "\n",
    "images = np.random.rand(num_images, image_size[1], image_size[0], num_channels)  # Note: OpenCV style (width, height)\n",
    "print(f\"IMAGE SHAPE: {images.shape}\") # 2 images, 64 by 64, 3 channels\n",
    "print(\"-----\")\n",
    "\n",
    "# Extract patches from images\n",
    "patches = extract_patches(images, patch_size)\n",
    "print(f\"PATCHES SHAPE: {patches.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Add positional encoding\n",
    "num_patches = patches.shape[0]\n",
    "patches_with_pos = positional_encoding(patches.reshape(num_patches, -1, num_channels))\n",
    "print(f\"PATCHES WITH POS: {patches_with_pos.shape}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Reshape patches_with_pos to match the expected input shape for linear layer\n",
    "patches_with_pos_reshaped = patches_with_pos.reshape(num_images, -1, dim)\n",
    "print(f\"PATCHES WITH POS RESHAPE: {patches_with_pos.shape}\")\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation to generate Q, K, and V matrices\n",
    "linear_qkv = nn.Linear(dim, d_k * 3, bias=False)\n",
    "qkv = linear_qkv(torch.tensor(patches_with_pos_reshaped, dtype=torch.float32))\n",
    "qkv = qkv.reshape(num_images, -1, 3, num_heads, d_k // num_heads).permute(2, 0, 3, 1, 4)\n",
    "Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "print(f\"Q:\\n{Q}\\nK:\\n{K}\\nV:\\n{V}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Calculate attention scores\n",
    "scale = d_k ** -0.5\n",
    "attention_scores = (torch.matmul(Q, K.transpose(-2, -1))) * scale\n",
    "print(f\"Attention scores (shape: {attention_scores.shape}):\\n{attention_scores}\")\n",
    "print(\"-----\")\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "print(f\"Attention weights (shape: {attention_weights.shape}):\\n{attention_weights}\")\n",
    "\n",
    "# Compute the weighted sum using attention weights\n",
    "output = torch.matmul(attention_weights, V)\n",
    "print(f\"Output after weighted sum - self-attention: (shape: {output.shape})\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"vit_small\" # num_heads = 6\n",
    "checkpoint_key = \"teacher\"\n",
    "patch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arch in torchvision_models.__dict__.keys():\n",
    "    model = (torchvision_models.__dict__[arch](\n",
    "        num_classes=0))\n",
    "    model.fc = nn.Identity()\n",
    "else:\n",
    "    model = vits.__dict__[arch](\n",
    "        patch_size=patch_size, num_classes=0)\n",
    "\n",
    "url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "\n",
    "state_dict = torch.hub.load_state_dict_from_url(\n",
    "    url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions <a class=\"anchor\" id=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = pth_transforms.Compose([\n",
    "    # pth_transforms.Resize((480, 480)),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(img_name, path, patch_size):\n",
    "    img_path = f\"{path}/{img_name}\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    print(f\"Original Image Size: {img.size}\")\n",
    "\n",
    "    img = transform(img)\n",
    "    print(f\"Transformed Image Size: {img.shape}\")\n",
    "        \n",
    "    # Make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "    print(f\"Image by Patch Size: {img.shape}\")\n",
    "    \n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    y, attentions, test_scores, test_weights = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1]  # Number of heads\n",
    "\n",
    "    # Keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    \n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "    \n",
    "    return y, attentions, test_scores, test_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Attentions Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img, img_per_col = 3, ):\n",
    "    num_images = len(img)\n",
    "    num_images\n",
    "\n",
    "    nrow = num_images // img_per_col\n",
    "    ncol = num_images // nrow\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrow, ncol,\n",
    "        gridspec_kw=dict(wspace=0.05, hspace=0.05,\n",
    "                         top=1. - 0.5 / (nrow + 1), bottom=0.5 / (nrow + 1),\n",
    "                         left=0.5 / (ncol + 1), right=1 - 0.5 / (ncol + 1)),\n",
    "        figsize=(ncol + 10, nrow + 10)\n",
    "    )\n",
    "\n",
    "    for i, image in enumerate(img):\n",
    "        ax = axes[i // ncol, i % ncol]\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Run <a class=\"anchor\" id=\"runs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = \"/usr/src/ai_tutorials/data/imagenet/val/n01440764\"\n",
    "img_name = \"ILSVRC2012_val_00000293.JPEG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(full_path + '/' + img_name)\n",
    "print(f\"IMG SHAPE: {img.size}\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run attention pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, img_attentions, scores, weights = get_attention(img_name=img_name, path=full_path, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_attentions), img_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION SCORES: torch.Size([1, 6, 714, 714])\n",
    "# tensor([[ 3.1271, -1.5889, -1.9582, -1.7589, -1.5342],\n",
    "#         [ 3.2784, -0.5752, -0.8337, -0.8466, -0.6552],\n",
    "#         [ 3.2076, -0.8093, -0.8502, -0.7562, -0.6061],\n",
    "#         [ 3.2198, -0.7407, -0.7294, -0.5392, -0.4825],\n",
    "#         [ 2.9159, -0.9545, -1.0094, -0.9100, -0.4406]], device='cuda:0')\n",
    "\n",
    "# ATTENTION WEIGHTS (SOFTMAX): torch.Size([1, 6, 714, 714])\n",
    "# tensor([[0.0636, 0.0006, 0.0004, 0.0005, 0.0006],\n",
    "#         [0.0644, 0.0014, 0.0011, 0.0010, 0.0013],\n",
    "#         [0.0659, 0.0012, 0.0011, 0.0013, 0.0015],\n",
    "#         [0.0625, 0.0012, 0.0012, 0.0015, 0.0015],\n",
    "#         [0.0478, 0.0010, 0.0009, 0.0010, 0.0017]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_score = scores[0][0][:5][:5]\n",
    "first_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute softmax along the last dimension (dim=1)\n",
    "attn_weights = first_score.softmax(dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_weight = weights[0][0][:5][:5]\n",
    "first_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img=img_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
